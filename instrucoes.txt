PASSO A PASSO GERAL DE COMO BAIXAR, ANALISAR E PREPARAR O DATASET PARA
TREINAMENTO DA REDE NEURAL.

Muito importante: executar os arquivos pela numeração para obter o melhor resultado.

===========================================================================

1. CONFIGURAÇÕES DO AMBIENTE E DOWNLOAD DO DATASET
--------------------------------------------------

1.1. Instalar a API do Kaggle:
     pip install kaggle

1.2. Criar o token de acesso no Kaggle:
     - Acessar kaggle.com → Perfil → Settings → Create New Token
     - Isso gera o arquivo "kaggle.json"
     - Salvar o arquivo em:
           C:\Users\<SEU-USUARIO>\.kaggle\kaggle.json
       (Se a pasta .kaggle não existir, criar manualmente)

1.3. Importar a API no código Python:
     from kaggle import api

1.4. Escolher o dataset:
     O link deve ser no formato:
     kaggle.com/datasets/<CRIADOR>/<NOME-DO-DATASET>

1.5. Definir constantes para download:
     - DATASET_REF = "<CRIADOR>/<NOME-DO-DATASET>"
     - DESTINO = "./dados_jogadores"

1.6. Rodar o arquivo de download (01_baixar_dataset.py)
     Isso criará a pasta de dados e salvará o CSV do Kaggle.

===========================================================================

2. ANÁLISE INICIAL DO DATASET (02_analise_inicial.py)
-----------------------------------------------------

2.1. Importar o pandas:
     import pandas as pd

2.2. Carregar o CSV original e visualizar informações:
     - df.head(): primeiros registros do dataset
     - df.columns: nome de todas as colunas
     - df.info(): tipos de dados, quantidades e uso de memória
     - df["League"].unique(): valores únicos da coluna "League"
     - df["Season"].unique(): valores únicos da coluna "Season"
     - df["Stage"].unique(): valores únicos da coluna "Stage"

2.3. O objetivo é compreender a estrutura do dataset original e
     identificar os valores corretos antes de realizar filtragens.

===========================================================================

3. FILTRAGEM DO DATASET (03_filtrar_dataset.py)
------------------------------------------------

Selecionamos apenas dados relevantes para o objetivo do projeto:

- League = NBA  
  (focamos na liga americana)

- Season = 2016 - 2017  
  (temporada escolhida por conter muitos dos principais jogadores em alto nível)

- Stage = Playoffs  
  (fase mata-mata, jogos decisivos, ideal para análise de impacto)

Após a filtragem, o dataset resultante contém apenas
jogadores que atuaram nos Playoffs da NBA 2016–2017.

A saída desse processo gerou o arquivo:
    ./dados_jogadores/nba_playoffs_2016_2017.csv

===========================================================================

4. SELEÇÃO DE CAMPOS (LIMPEZA DO DATASET)
-----------------------------------------

Algumas colunas foram removidas, pois não contribuem para o aprendizado do
modelo e podem introduzir ruído. Outras foram mantidas por terem importância
estatística ou contextual.

-----------------------------------------------------------
CAMPOS REMOVIDOS (não úteis para análise de impacto)
-----------------------------------------------------------

- Altura e peso (height, weight):
  Apesar de serem atributos físicos, não representam impacto direto no jogo.
  Rebotes, roubos e tocos já capturam melhor a performance do atleta.

- birth_year, birth_month, birth_date
  (informações biográficas irrelevantes)

- nationality, high_school
  (não influenciam desempenho em jogo)

- draft_round, draft_pick, draft_team
  (histórico de draft não impacta cenários de clutch)

-----------------------------------------------------------
CAMPOS MANTIDOS (úteis para análise e métricas)
-----------------------------------------------------------

- League, Season, Stage, Player, Team (contextualizam o jogador)
- GP (jogos disputados)
- MIN (minutos jogados)
- FGM, FGA (arremessos convertidos e tentados → FG%)
- 3PM, 3PA (bolas de 3 convertidas e tentadas → 3P%)
- FTM, FTA (lances livres convertidos e tentados → FT%)
- ORB, DRB, REB (rebotes ofensivos, defensivos e totais)
- AST (assistências)
- STL (roubos)
- BLK (tocos)
- PTS (pontos)
- TOV (turnovers)
- PF (faltas)

Essas estatísticas descrevem objetivamente o impacto real do jogador
em quadra, especialmente no contexto de viradas e decisões.

===========================================================================

5. CRIAÇÃO DE FEATURES PROPORCIONAIS E PERCENTUAIS (FEATURE ENGINEERING)
-------------------------------------------------------------------------

Após a limpeza, realizamos a etapa de "feature engineering".
Transformamos estatísticas brutas em métricas proporcionais, percentuais e
indicadores de eficiência. Isso permite que a rede neural receba dados
mais significativos e comparáveis entre jogadores com diferentes tempos
de quadra.

-----------------------------------------------------------
A) Métricas Por Minuto (Proporcionais)
-----------------------------------------------------------

Criadas dividindo o valor total pelo tempo em quadra (MIN):

- PTS_per_min = PTS / MIN
- REB_per_min = REB / MIN
- AST_per_min = AST / MIN
- STL_per_min = STL / MIN
- BLK_per_min = BLK / MIN
- TOV_per_min = TOV / MIN

Essas métricas permitem comparar jogadores de forma justa mesmo que
tenham jogado minutos diferentes.

-----------------------------------------------------------
B) Aproveitamentos Percentuais
-----------------------------------------------------------

- FG_pct = FGM / FGA   (Field Goal %)
- 3P_pct = 3PM / 3PA   (% de acerto em bolas de 3)
- FT_pct = FTM / FTA   (lances livres)

Divisões por zero foram tratadas com fillna(0).

-----------------------------------------------------------
C) Objetivo
-----------------------------------------------------------

Criar um conjunto de features matematicamente relevantes que representem:

- Eficiência ofensiva e defensiva
- Produção real por minuto
- Risco (turnovers)
- Capacidade de impacto em clutch

-----------------------------------------------------------
D) Salvamento
-----------------------------------------------------------

As features foram salvas em:

    ./dados_jogadores/nba_playoffs_2017_2017_features.csv

Dataset final preparado para a próxima etapa:
NORMALIZAÇÃO E TREINAMENTO DA REDE NEURAL.

===========================================================================

6. NORMALIZAÇÃO DOS DADOS + CRIAÇÃO DA EFICIÊNCIA (EFF)
   (06_normalizacao_dados.py)
--------------------------------------------------------

Após a criação das features proporcionais e percentuais, realizamos a etapa de
normalização dos dados. Essa etapa é fundamental para preparar o dataset para o
treinamento da rede neural, garantindo que todas as entradas tenham escalas
compatíveis.

Utilizamos o método MinMaxScaler (0–1), que transforma todas as colunas
numéricas para o intervalo [0, 1]. Isso evita que valores maiores influenciem
de forma desproporcional o aprendizado do modelo.

-----------------------------------------------------------
A) SELEÇÃO DAS FEATURES NUMÉRICAS
-----------------------------------------------------------

Como o dataset final contém apenas estatísticas numéricas, utilizamos:

    df_numerico = df.select_dtypes(include=['float64', 'int64'])

Isso garante que apenas colunas relevantes sejam normalizadas.

-----------------------------------------------------------
B) APLICAÇÃO DO MINMAXSCALER (0–1)
-----------------------------------------------------------

1. Criamos o scaler:
       scaler = MinMaxScaler()

2. Ajustamos e transformamos:
       dados_normalizados = scaler.fit_transform(df_numerico)

O resultado é uma matriz com todas as features convertidas para a escala 0–1.

-----------------------------------------------------------
C) RECONSTRUÇÃO DO DATAFRAME NORMALIZADO
-----------------------------------------------------------

    df_normalizado = pd.DataFrame(
        dados_normalizados,
        columns=df_numerico.columns
    )

Isso garante que as colunas originais sejam preservadas com seus nomes corretos.

-----------------------------------------------------------
D) CRIAÇÃO DA MÉTRICA DE EFICIÊNCIA (EFF)
-----------------------------------------------------------

Após a normalização, criamos a métrica EFF, uma medida de impacto total do
jogador. A fórmula utilizada segue o padrão da NBA, adaptada para valores
normalizados:

    EFF = (PTS + REB + AST + STL + BLK)
          - ((FGA - FGM) + (FTA - FTM) + TOV_per_min)

Essa métrica combina aspectos ofensivos, defensivos e penalidades, servindo como
o target (variável de saída) para o treinamento da rede neural.

-----------------------------------------------------------
E) SALVAMENTO DO DATASET FINAL NORMALIZADO
-----------------------------------------------------------

O dataset final, com todas as features normalizadas e a coluna EFF criada, foi
salvo em:

    ./dados_jogadores/nba_playoffs_2016_2017_normalizado.csv

Este é o arquivo oficial utilizado na etapa 07 (treinamento da rede neural).

-----------------------------------------------------------
F) OBJETIVO DESSA ETAPA
-----------------------------------------------------------

A normalização:
- evita que features de maior magnitude dominem o treinamento;
- melhora a estabilidade e velocidade de convergência da rede neural;
- permite que todas as métricas funcionem em uma mesma escala;
- prepara o dataset para ser usado diretamente como entrada no modelo.

===========================================================================

7. TREINAMENTO DO MODELO DE REDE NEURAL (PyTorch)
   (07_treinamento_rede.py)
-----------------------------------------------------------

Após as etapas de criação de features e normalização, iniciou-se o
treinamento da rede neural MLP utilizando a biblioteca PyTorch,
conforme exigido pelos requisitos do projeto.

O objetivo é prever a eficiência (EFF) dos jogadores com base em suas
estatísticas normalizadas.

-----------------------------------------------------------
A) PREPARAÇÃO DOS DADOS PARA TREINO
-----------------------------------------------------------

Foram executados os passos:

1. Separação das variáveis de entrada (X) e saída (y):
       X = todas as features numéricas normalizadas
       y = coluna EFF

2. Divisão dos dados em treino e teste:
       80% para treino, 20% para teste

3. Conversão para tensores PyTorch:
       torch.tensor(..., dtype=torch.float32)

Isso permite que os dados sejam usados diretamente na rede neural.

-----------------------------------------------------------
B) ARQUITETURA DA REDE NEURAL (MLP)
-----------------------------------------------------------

A rede utilizada possui o formato:

    Entrada (26 features)
       ↓
    Camada oculta 1 — 64 neurônios + função de ativação
       ↓
    Camada oculta 2 — 32 neurônios + função de ativação
       ↓
    Saída — 1 neurônio (valor da EFF prevista)

As funções de ativação foram parametrizadas, permitindo testar ReLU
e Tanh conforme exigido no projeto.

A camada de saída é linear, pois o problema é de regressão.

-----------------------------------------------------------
C) LOOP DE TREINAMENTO
-----------------------------------------------------------

Foi implementada a função `treinar_modelo`, contendo:

- função de perda: MSELoss  
- otimizador: Adam  
- taxa de aprendizado (η): 0.001  
- 200 épocas de treino  
- tamanho do lote: 16 (mini-batches)

Durante o treinamento, o modelo faz:

    forward → cálculo da perda → backpropagation → atualização dos pesos

A cada 20 épocas é exibido o erro médio, atendendo ao requisito:

> “Exibir o vetor de erros a cada iteração.”

-----------------------------------------------------------
D) TESTE DAS FUNÇÕES DE ATIVAÇÃO
-----------------------------------------------------------

Foram treinados dois modelos:

- MLP + ReLU
- MLP + Tanh

O desempenho foi monitorado separadamente e os vetores de perda foram
armazenados para geração dos gráficos.

===========================================================================

8. AVALIAÇÃO DO MODELO E CURVA DE PERDA
-----------------------------------------------------------

Após o treinamento, as curvas de perda das duas funções de ativação
foram plotadas em um mesmo gráfico utilizando Matplotlib.  
Esse gráfico demonstra a velocidade de convergência e estabilidade
de cada modelo.

-----------------------------------------------------------
A) CURVA DE PERDA (TRAIN LOSS)
-----------------------------------------------------------

O gráfico gerado contém:

- eixo X: número de épocas  
- eixo Y: perda média (MSE)  
- duas curvas: ReLU e Tanh  

A ReLU apresentou convergência mais rápida e menor perda final,
o que confirma seu bom desempenho em redes profundas.

-----------------------------------------------------------
B) AVALIAÇÃO NO CONJUNTO DE TESTE
-----------------------------------------------------------

Para verificar a capacidade de generalização, foram calculadas:

- MSE (Mean Squared Error)
- R² (coeficiente de determinação)

Resultados obtidos:

    ReLU → MSE = 0.001007 | R² = 0.9971  
    Tanh → MSE = 0.001708 | R² = 0.9951  

Esse desempenho indica que o modelo explica aproximadamente 99% da
variação da eficiência real dos jogadores no conjunto de teste,
sendo adequado para previsões futuras.

A ReLU foi escolhida como modelo final por apresentar:
- menor perda,
- maior R²,
- e maior estabilidade durante o treinamento.

===========================================================================